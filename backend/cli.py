"""
å‘½ä»¤è¡Œå…¥å£
"""
import os
# åœ¨å¯¼å…¥ä»»ä½•ç½‘ç»œåº“ä¹‹å‰ï¼Œå…ˆæ¸…é™¤ä»£ç†è®¾ç½®
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('all_proxy', None)
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('ALL_PROXY', None)

import argparse
import concurrent.futures
import datetime
import logging
from typing import List
from backend.core.config import Config
from backend.core.logging import setup_logging, get_logger
from backend.storage import init_db, PaperRepository
from backend.sources import (
    BioRxivSource, PubMedSource, RSSSource, EuropePMCSource,
    ScienceNewsSource, GitHubSource, SemanticScholarSource
)
from backend.core.ranking import rank_and_select, get_item_id
from backend.llm import generate_daily_report, generate_final_summary
from backend.push import PushPlusSender, EmailSender, WeComSender

logger = get_logger(__name__)


def run_push_task(window_days: int = None, top_k: int = None):
    """æ‰§è¡Œæ¨é€ä»»åŠ¡"""
    window_days = window_days or Config.DEFAULT_WINDOW_DAYS
    top_k = top_k or Config.TOP_K
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    repo = PaperRepository()
    
    # åˆ›å»ºè¿è¡Œè®°å½•ï¼ˆåœ¨é…ç½®æ—¥å¿—ä¹‹å‰ï¼Œä»¥ä¾¿æ—¥å¿—æ–‡ä»¶ååŒ…å« run_idï¼‰
    run_id = repo.create_run(window_days)
    
    # é‡æ–°é…ç½®æ—¥å¿—ï¼Œä½¿ç”¨ run_id ç”Ÿæˆç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶
    setup_logging(run_id=run_id)
    logger = get_logger(__name__)
    
    logger.info("=" * 80)
    logger.info("å¼€å§‹æ‰§è¡Œç”Ÿç‰©åŒ–å­¦ç ”ç©¶èµ„è®¯æŠ“å–ä¸æ¨é€")
    logger.info(f"æŠ“å–çª—å£ï¼š{window_days}å¤©ï¼ˆEuropePMC {Config.EUROPEPMC_WINDOW_DAYS}å¤©ï¼‰ï¼Œå…¶ä½™æ•°æ®æºå‡ä¸ºè¿‘{window_days}å¤©")
    logger.info("=" * 80)
    logger.info(f"è¿è¡ŒID: {run_id}")
    
    try:
        # è·å–å·²å¤„ç†çš„è®ºæ–‡IDï¼ˆå»é‡ç”¨ï¼‰ï¼Œé¿å…å¤šæ¬¡ç‚¹å‡»å¯¼è‡´é‡å¤è®¡æ•°
        logger.info("æ­£åœ¨è·å–å·²å¤„ç†è®ºæ–‡IDä»¥è¿›è¡Œå»é‡...")
        sent_ids = repo.get_sent_ids()
        logger.info(f"å·²å¤„ç† {len(sent_ids)} ç¯‡è®ºæ–‡")
        
        # å®šä¹‰æ•°æ®æº
        # æ³¨ï¼šæš‚æ—¶ç¦ç”¨ Semantic Scholarï¼ˆAPIé™æµä¸¥é‡ï¼‰å’Œ ScienceNewsï¼ˆRSSæºå¤±æ•ˆï¼‰
        sources = [
            BioRxivSource(window_days),
            PubMedSource(window_days),
            RSSSource(window_days),
            EuropePMCSource(Config.EUROPEPMC_WINDOW_DAYS),
            # ScienceNewsSource(window_days),  # æš‚æ—¶ç¦ç”¨ï¼šRSSæºè¿”å›404
            GitHubSource(window_days),
            # SemanticScholarSource(window_days),  # æš‚æ—¶ç¦ç”¨ï¼šAPIé™æµä¸¥é‡ï¼Œå¯¼è‡´è¶…æ—¶
        ]
        
        # å¹¶å‘æŠ“å–ï¼ˆä¼ å…¥å·²å­˜åœ¨çš„IDè¿›è¡Œå»é‡ï¼Œé¿å…é‡å¤è®¡æ•°ï¼‰
        logger.info("\nå¼€å§‹å¹¶å‘æŠ“å–æ•°æ®...")
        source_results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(sources)) as executor:
            future_to_source = {
                # ä¼ å…¥ repo.get_sent_ids() è·å–çš„é›†åˆ
                executor.submit(source.fetch, sent_ids, Config.EXCLUDE_KEYWORDS): source
                for source in sources
            }
            
            for future in concurrent.futures.as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    result = future.result()
                    source_results.append(result)
                    logger.info(f"{source.name}: è·å–åˆ° {len(result.papers)} æ¡ç»“æœ")
                except Exception as e:
                    logger.error(f"{source.name} æœç´¢å¤±è´¥: {e}")
                    source_results.append(
                        type('SourceResult', (), {'source_name': source.name, 'papers': [], 'error': str(e)})()
                    )
        
        # åˆå¹¶ä¸ç­›é€‰
        logger.info("\nå¼€å§‹åˆå¹¶ä¸ç­›é€‰...")
        
        # è·å–æ‰€æœ‰è®ºæ–‡ï¼ˆç”¨äºè¯„åˆ†ï¼Œä¸è¿›è¡Œå»é‡ï¼‰
        all_papers = []
        for result in source_results:
            if result.success():
                all_papers.extend(result.papers)
        
        # å¯¹å½“å¤©æ‰€æœ‰è®ºæ–‡è¿›è¡Œè¯„åˆ†
        logger.info(f"\nå¯¹å½“å¤©æ‰€æœ‰è®ºæ–‡è¿›è¡Œè¯„åˆ†ï¼ˆå…±{len(all_papers)}ç¯‡ï¼‰...")
        from backend.core.scoring import score_paper
        all_scored_papers = [score_paper(p) for p in all_papers]
        
        # æŒ‰è¯„åˆ†æ’åº
        all_scored_papers.sort(key=lambda x: x.score, reverse=True)
        
        if not all_scored_papers:
            logger.info("å½“å¤©æ²¡æœ‰æ–°è®ºæ–‡éœ€è¦æ¨é€")
            repo.update_run(run_id, status='completed')
            return
        
        # å¿«é€ŸAIé¢„ç­›é€‰ï¼šå¯¹é«˜åˆ†è®ºæ–‡è¿›è¡Œå¿«é€Ÿåˆ¤æ–­
        logger.info(f"\nå¼€å§‹å¿«é€ŸAIé¢„ç­›é€‰ï¼ˆå¯¹é«˜åˆ†è®ºæ–‡è¿›è¡Œç›¸å…³æ€§åˆ¤æ–­ï¼‰...")
        from backend.llm.quick_check import quick_relevance_check
        from backend.models import ScoreReason
        
        RELEVANCE_CHECK_THRESHOLD = Config.QUICK_FILTER_THRESHOLD  # ä»é…ç½®è¯»å–é˜ˆå€¼
        # åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼šè®ºæ–‡æ•°é‡å°‘æ—¶é™ä½é˜ˆå€¼ï¼Œé¿å…æ¼æ‰ç›¸å…³è®ºæ–‡
        if len(all_scored_papers) <= 5:
            RELEVANCE_CHECK_THRESHOLD = max(RELEVANCE_CHECK_THRESHOLD - 15, 20)
            logger.info(f"è®ºæ–‡æ•°é‡è¾ƒå°‘ï¼ˆ{len(all_scored_papers)}ç¯‡ï¼‰ï¼ŒåŠ¨æ€é™ä½ç­›é€‰é˜ˆå€¼è‡³ {RELEVANCE_CHECK_THRESHOLD}åˆ†")
        elif len(all_scored_papers) <= 10:
            RELEVANCE_CHECK_THRESHOLD = max(RELEVANCE_CHECK_THRESHOLD - 10, 30)
            logger.info(f"è®ºæ–‡æ•°é‡è¾ƒå°‘ï¼ˆ{len(all_scored_papers)}ç¯‡ï¼‰ï¼ŒåŠ¨æ€é™ä½ç­›é€‰é˜ˆå€¼è‡³ {RELEVANCE_CHECK_THRESHOLD}åˆ†")
        logger.info(f"å¿«é€Ÿç­›é€‰é˜ˆå€¼: {RELEVANCE_CHECK_THRESHOLD}åˆ†ï¼ˆåªå¯¹â‰¥{RELEVANCE_CHECK_THRESHOLD}åˆ†çš„è®ºæ–‡è¿›è¡ŒAIåˆ¤æ–­ï¼‰")
        filtered_papers = []
        filtered_count = 0
        checked_count = 0
        
        for scored_paper in all_scored_papers:
            if scored_paper.score >= RELEVANCE_CHECK_THRESHOLD:
                checked_count += 1
                # å¯¹é«˜åˆ†è®ºæ–‡è¿›è¡Œå¿«é€ŸAIåˆ¤æ–­
                is_relevant = quick_relevance_check(scored_paper.paper)
                
                if is_relevant is False:
                    # ä¸ç›¸å…³ï¼Œç›´æ¥è¿‡æ»¤ï¼ˆä¸æ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼‰
                    logger.info(f"â­ï¸ [å¿«é€Ÿç­›é€‰] è®ºæ–‡ '{scored_paper.paper.title[:60]}...' (è¯„åˆ†: {scored_paper.score:.1f}) è¢«åˆ¤æ–­ä¸ºä¸ç›¸å…³ï¼Œå·²ç›´æ¥è¿‡æ»¤")
                    filtered_count += 1
                    continue  # è·³è¿‡ï¼Œä¸æ·»åŠ åˆ°filtered_papers
                elif is_relevant is None:
                    # åˆ¤æ–­å¤±è´¥ï¼Œä¿ç•™è®ºæ–‡ï¼ˆä¿å®ˆç­–ç•¥ï¼Œé¿å…è¯¯è¿‡æ»¤ï¼‰
                    logger.warning(f"âš ï¸ [å¿«é€Ÿç­›é€‰] è®ºæ–‡ '{scored_paper.paper.title[:60]}...' AIåˆ¤æ–­å¤±è´¥ï¼Œä¿ç•™è®ºæ–‡ï¼ˆä¿å®ˆç­–ç•¥ï¼‰")
                    filtered_papers.append(scored_paper)
                else:
                    # ç›¸å…³ï¼Œä¿ç•™
                    filtered_papers.append(scored_paper)
            else:
                # ä½åˆ†è®ºæ–‡ï¼Œç›´æ¥ä¿ç•™ï¼ˆä¸è¿›è¡Œå¿«é€Ÿæ£€æŸ¥ï¼ŒèŠ‚çœAPIè°ƒç”¨ï¼‰
                filtered_papers.append(scored_paper)
        
        # é‡æ–°æ’åº
        filtered_papers.sort(key=lambda x: x.score, reverse=True)
        all_scored_papers = filtered_papers
        
        if checked_count > 0:
            logger.info(f"âœ… [å¿«é€Ÿç­›é€‰] å®Œæˆï¼šæ£€æŸ¥äº† {checked_count} ç¯‡é«˜åˆ†è®ºæ–‡ï¼Œè¿‡æ»¤äº† {filtered_count} ç¯‡ä¸ç›¸å…³è®ºæ–‡ï¼Œä¿ç•™äº† {len(all_scored_papers)} ç¯‡è®ºæ–‡")
        
        # ä¿å­˜æ‰€æœ‰è®ºæ–‡çš„è¯„åˆ†
        logger.info(f"ä¿å­˜æ‰€æœ‰è®ºæ–‡çš„è¯„åˆ†ï¼ˆå…±{len(all_scored_papers)}ç¯‡ï¼‰...")
        repo.save_scores(run_id, all_scored_papers)
        
        # æ›´æ–°è¿è¡Œè®°å½•
        repo.update_run(
            run_id,
            total_papers=sum(len(r.papers) for r in source_results),
            unseen_papers=len(all_papers),
            top_k=len(all_scored_papers),  # æ‰€æœ‰è®ºæ–‡éƒ½å¤„ç†
            status='running'
        )
        
        # å•ç¯‡å¤„ç†æ¨¡å¼ï¼šä¸€ç¯‡ä¸€ç¯‡å¤„ç†
        logger.info(f"\nå¼€å§‹å•ç¯‡å¤„ç†æ¨¡å¼ï¼šå…± {len(all_scored_papers)} ç¯‡è®ºæ–‡ï¼Œé€ç¯‡å¤„ç†")
        
        all_paper_reports = []
        processed_papers = []  # è®°å½•æˆåŠŸå¤„ç†çš„è®ºæ–‡
        
        for paper_idx, scored_paper in enumerate(all_scored_papers, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"å¤„ç†è®ºæ–‡ {paper_idx}/{len(all_scored_papers)}")
            logger.info(f"æ ‡é¢˜: {scored_paper.paper.title[:80]}...")
            logger.info(f"è¯„åˆ†: {scored_paper.score:.1f}åˆ†")
            logger.info(f"{'='*80}\n")
            
            try:
                # ç”Ÿæˆå•ç¯‡è®ºæ–‡æŠ¥å‘Šï¼ˆåŒ…å«é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ï¼‰
                from backend.llm.generator import generate_single_paper_report
                paper_report = generate_single_paper_report(scored_paper, paper_idx)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é™çº§æŠ¥å‘Š
                is_fallback = paper_report and paper_report.startswith("## âš ï¸ æŠ¥å‘Šç”Ÿæˆè¯´æ˜")
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸ç›¸å…³è®ºæ–‡
                is_irrelevant = paper_report and paper_report.startswith("## ä¸ç›¸å…³è®ºæ–‡")
                
                if is_irrelevant:
                    # ä¸ç›¸å…³è®ºæ–‡ï¼Œæ·»åŠ åˆ°æŠ¥å‘Šä¸­ä½†å•ç‹¬æ ‡è®°ï¼ˆä¸æ ‡è®°ä¸ºå·²æ¨é€ï¼‰
                    all_paper_reports.append(paper_report)
                    logger.info(f"â­ï¸ è®ºæ–‡ {paper_idx} ä¸å±äºä¸‰å¤§ç ”ç©¶æ–¹å‘ï¼Œå·²æ·»åŠ åˆ°æŠ¥å‘Šä½†æ ‡è®°ä¸ºä¸ç›¸å…³")
                elif paper_report and not is_fallback:
                    # æˆåŠŸç”ŸæˆAIæŠ¥å‘Š
                    all_paper_reports.append(paper_report)
                    # è®°å½•æˆåŠŸå¤„ç†çš„è®ºæ–‡
                    processed_papers.append(scored_paper)
                    logger.info(f"âœ… è®ºæ–‡ {paper_idx} å¤„ç†æˆåŠŸï¼ˆAIåˆ†æå®Œæˆï¼‰")
                else:
                    # é™çº§æŠ¥å‘Šï¼ˆAPIè°ƒç”¨å¤±è´¥ï¼‰ï¼Œä»ç„¶æ·»åŠ åˆ°æŠ¥å‘Šä¸­ï¼Œä½†ä¸æ ‡è®°ä¸ºå·²æ¨é€
                    all_paper_reports.append(paper_report)
                    logger.warning(f"âš ï¸ è®ºæ–‡ {paper_idx} ä½¿ç”¨é™çº§æŠ¥å‘Šï¼ˆAPIè°ƒç”¨å¤±è´¥ï¼‰ï¼Œè®ºæ–‡å°†ä¸ä¼šæ ‡è®°ä¸ºå·²æ¨é€ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šé‡æ–°å¤„ç†")
                
            except Exception as e:
                logger.error(f"âŒ è®ºæ–‡ {paper_idx} å¤„ç†å¤±è´¥: {e}", exc_info=True)
                # å³ä½¿å¤±è´¥ï¼Œä¹Ÿç”Ÿæˆä¸€ä¸ªç®€å•çš„é™çº§æŠ¥å‘Š
                from backend.llm.generator import _generate_fallback_report
                fallback_report = _generate_fallback_report([scored_paper], e)
                all_paper_reports.append(fallback_report)
                # å¤±è´¥çš„è®ºæ–‡ä¸æ ‡è®°ä¸ºå·²æ¨é€ï¼Œä¸‹æ¬¡ä¼šé‡æ–°å¤„ç†
                logger.warning(f"è®ºæ–‡ {paper_idx} å°†ä¸ä¼šæ ‡è®°ä¸ºå·²æ¨é€ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šé‡æ–°å¤„ç†")
        
        # ç›´æ¥åˆå¹¶æ‰€æœ‰å•ç¯‡æŠ¥å‘Šï¼ˆä¸ç”Ÿæˆæœ€ç»ˆæ€»ç»“ï¼‰
        logger.info(f"\n{'='*80}")
        logger.info("æ‰€æœ‰è®ºæ–‡å¤„ç†å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        logger.info(f"{'='*80}\n")
        
        # åˆ†ç¦»ç›¸å…³è®ºæ–‡å’Œä¸ç›¸å…³è®ºæ–‡
        relevant_reports = []
        irrelevant_reports = []
        
        for report in all_paper_reports:
            if report.startswith("## ä¸ç›¸å…³è®ºæ–‡"):
                irrelevant_reports.append(report)
            else:
                relevant_reports.append(report)
        
        # æ„å»ºæŠ¥å‘Š
        if relevant_reports or irrelevant_reports:
            daily_report = "## ğŸ“Š ä»Šæ—¥ç ”ç©¶æ€»ç»“\n\n"
            
            # ç›¸å…³è®ºæ–‡éƒ¨åˆ†
            if relevant_reports:
                daily_report += f"### âœ… ç›¸å…³è®ºæ–‡ï¼ˆå…± {len(relevant_reports)} ç¯‡ï¼‰\n\n"
                # è§„èŒƒåŒ–æŠ¥å‘Šæ ¼å¼ï¼šä¿®å¤æ ‡é¢˜æ ¼å¼ä¸ä¸€è‡´é—®é¢˜ï¼ˆ## æ”¹ä¸º ###ï¼‰
                normalized_reports = []
                for report in relevant_reports:
                    # ä¿®å¤æ ‡é¢˜æ ¼å¼ï¼šå°† "## ã€è®ºæ–‡æ ‡é¢˜ã€‘" æ”¹ä¸º "### ã€è®ºæ–‡æ ‡é¢˜ã€‘"
                    normalized_report = report.replace("## ã€è®ºæ–‡æ ‡é¢˜ã€‘", "### ã€è®ºæ–‡æ ‡é¢˜ã€‘")
                    normalized_reports.append(normalized_report)
                daily_report += "\n\n".join([f"## è®ºæ–‡ {i} é‡è¦ç ”ç©¶æˆæœ\n\n{report}" 
                                            for i, report in enumerate(normalized_reports, 1)])
            
            # ä¸ç›¸å…³è®ºæ–‡éƒ¨åˆ†
            if irrelevant_reports:
                if relevant_reports:
                    daily_report += "\n\n---\n\n"
                daily_report += f"### â­ï¸ å·²è¿‡æ»¤è®ºæ–‡ï¼ˆå…± {len(irrelevant_reports)} ç¯‡ï¼Œä¸å±äºä¸‰å¤§ç ”ç©¶æ–¹å‘ï¼‰\n\n"
                # è§„èŒƒåŒ–ä¸ç›¸å…³è®ºæ–‡æ ¼å¼ï¼šå°† "## ä¸ç›¸å…³è®ºæ–‡" æ”¹ä¸º "### ä¸ç›¸å…³è®ºæ–‡"
                normalized_irrelevant = []
                for report in irrelevant_reports:
                    normalized = report.replace("## ä¸ç›¸å…³è®ºæ–‡", "### ä¸ç›¸å…³è®ºæ–‡")
                    normalized = normalized.replace("## ã€è®ºæ–‡æ ‡é¢˜ã€‘", "### ã€è®ºæ–‡æ ‡é¢˜ã€‘")
                    normalized_irrelevant.append(normalized)
                daily_report += "\n\n".join([f"## å·²è¿‡æ»¤è®ºæ–‡ {i}\n\n{report}"
                                            for i, report in enumerate(normalized_irrelevant, 1)])
        else:
            daily_report = "## ğŸ“Š ä»Šæ—¥ç ”ç©¶æ€»ç»“\n\næœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æœªå‘ç°ç›¸å…³è®ºæ–‡ã€‚"
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ï¼ˆä¼ å…¥åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯ï¼‰
        save_report_to_file(daily_report, len(all_scored_papers), source_results, run_id,
                            relevant_count=len(relevant_reports), irrelevant_count=len(irrelevant_reports))
        
        # æ¨é€æœ€ç»ˆæŠ¥å‘Šï¼ˆä¸æ ‡è®°å·²æ¨é€ï¼Œæ¯æ¬¡è¿è¡Œå†…å®¹ä¸€è‡´ï¼‰
        logger.info("\nå¼€å§‹æ¨é€æœ€ç»ˆæŠ¥å‘Š...")
        push_success = False
        
        # PushPlus
        pushplus_sender = PushPlusSender()
        if pushplus_sender.send("ç”Ÿç‰©åŒ–å­¦ç ”ç©¶ç²¾é€‰è®ºæ–‡", daily_report):
            push_success = True
        
        # é‚®ä»¶
        email_sender = EmailSender()
        if email_sender.send("ç”Ÿç‰©åŒ–å­¦ç ”ç©¶ç²¾é€‰è®ºæ–‡", daily_report):
            push_success = True
        
        # ä¼ä¸šå¾®ä¿¡
        wecom_sender = WeComSender()
        if wecom_sender.send("ç”Ÿç‰©åŒ–å­¦ç ”ç©¶ç²¾é€‰è®ºæ–‡", daily_report):
            push_success = True
        
        logger.info(f"\næ¨é€å®Œæˆï¼šå¤„ç† {len(processed_papers)} ç¯‡è®ºæ–‡ï¼Œå…± {len(all_paper_reports)} ç¯‡è®ºæ–‡æŠ¥å‘Šï¼ˆæœªæ ‡è®°å·²æ¨é€ï¼Œä¸‹æ¬¡è¿è¡Œå°†é‡æ–°å¤„ç†ï¼‰")
        
        # æ›´æ–°è¿è¡Œè®°å½•
        repo.update_run(run_id, status='completed' if push_success else 'failed')
        
        logger.info("\n" + "=" * 80)
        logger.info("æ‰§è¡Œå®Œæˆï¼")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        repo.update_run(run_id, status='failed', error=str(e))
        raise


def save_report_to_file(content: str, papers_count: int, source_results: List, run_id: str,
                        relevant_count: int = 0, irrelevant_count: int = 0):
    """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    from pathlib import Path

    today = datetime.date.today().strftime('%Y%m%d')
    filename = f"data/reports/ç”ŸåŒ–é¢†åŸŸçªç ´æ±‡æ€»_{today}.txt"

    try:
        # ç¡®ä¿ reports ç›®å½•å­˜åœ¨
        report_path = Path(filename)
        report_path.parent.mkdir(parents=True, exist_ok=True)

        sources_info = [f"{r.source_name}({len(r.papers)})" for r in source_results]
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¿è¡ŒID: {run_id}\n")
            f.write(f"è®ºæ–‡æ€»æ•°: {papers_count}ï¼ˆç›¸å…³: {relevant_count} ç¯‡ï¼Œå·²è¿‡æ»¤: {irrelevant_count} ç¯‡ï¼‰\n")
            f.write(f"æ•°æ®æº: {', '.join(sources_info)}\n")
            f.write("=" * 80 + "\n\n")
            f.write(content)
        logger.info(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


def test_sources(source_name: str = None):
    """
    æµ‹è¯•æ•°æ®æº
    
    Args:
        source_name: è¦æµ‹è¯•çš„æ•°æ®æºåç§°ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸º Noneï¼Œåˆ™æµ‹è¯•æ‰€æœ‰æ•°æ®æºã€‚
                    å¯é€‰å€¼: 'biorxiv', 'pubmed', 'rss', 'europepmc', 'sciencenews', 
                           'github', 'semanticscholar'
    """
    logger.info("=" * 80)
    if source_name:
        logger.info(f"å¼€å§‹æµ‹è¯•æ•°æ®æº: {source_name}")
    else:
        logger.info("å¼€å§‹æµ‹è¯•æ‰€æœ‰æ•°æ®æº")
    logger.info("=" * 80)
    
    init_db()
    # ä¸è¿›è¡Œå»é‡ï¼Œæµ‹è¯•æ—¶å¤„ç†æ‰€æœ‰æ£€ç´¢åˆ°çš„è®ºæ–‡
    logger.info("æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ£€ç´¢åˆ°çš„è®ºæ–‡ï¼ˆä¸è¿›è¡Œå»é‡ï¼‰")
    
    # å®šä¹‰æ‰€æœ‰æ•°æ®æº
    all_sources = {
        'biorxiv': ("bioRxiv", BioRxivSource(Config.DEFAULT_WINDOW_DAYS)),
        'pubmed': ("PubMed", PubMedSource(Config.DEFAULT_WINDOW_DAYS)),
        'rss': ("RSS", RSSSource(Config.DEFAULT_WINDOW_DAYS)),
        'europepmc': ("Europe PMC", EuropePMCSource(Config.EUROPEPMC_WINDOW_DAYS)),
        'sciencenews': ("Science News", ScienceNewsSource(Config.DEFAULT_WINDOW_DAYS)),
        'github': ("GitHub", GitHubSource(Config.DEFAULT_WINDOW_DAYS)),
        'semanticscholar': ("Semantic Scholar", SemanticScholarSource(Config.DEFAULT_WINDOW_DAYS)),
    }
    
    # å¦‚æœæŒ‡å®šäº†æ•°æ®æºï¼Œåªæµ‹è¯•è¯¥æ•°æ®æº
    if source_name:
        source_name_lower = source_name.lower()
        if source_name_lower not in all_sources:
            logger.error(f"æœªçŸ¥çš„æ•°æ®æº: {source_name}")
            logger.info(f"å¯ç”¨çš„æ•°æ®æº: {', '.join(all_sources.keys())}")
            return
        sources = [(all_sources[source_name_lower][0], all_sources[source_name_lower][1])]
    else:
        # æµ‹è¯•æ‰€æœ‰æ•°æ®æºï¼ˆæ’é™¤å·²ç¦ç”¨çš„ï¼‰
        sources = [
            ("bioRxiv", BioRxivSource(Config.DEFAULT_WINDOW_DAYS)),
            ("PubMed", PubMedSource(Config.DEFAULT_WINDOW_DAYS)),
            ("RSS", RSSSource(Config.DEFAULT_WINDOW_DAYS)),
            ("Europe PMC", EuropePMCSource(Config.EUROPEPMC_WINDOW_DAYS)),
            # ("Science News", ScienceNewsSource(Config.DEFAULT_WINDOW_DAYS)),  # æš‚æ—¶ç¦ç”¨ï¼šRSSæºå¤±æ•ˆ
            ("GitHub", GitHubSource(Config.DEFAULT_WINDOW_DAYS)),
            # ("Semantic Scholar", SemanticScholarSource(Config.DEFAULT_WINDOW_DAYS)),  # æš‚æ—¶ç¦ç”¨ï¼šAPIé™æµä¸¥é‡
        ]
    
    results = {}
    for name, source in sources:
        logger.info(f"\næµ‹è¯•æ•°æ®æº: {name}")
        logger.info(f"  æ—¶é—´çª—å£: {source.window_days} å¤©")
        if hasattr(source, 'max_pages'):
            logger.info(f"  æœ€å¤§é¡µæ•°: {source.max_pages}")
        
        try:
            result = source.fetch(set(), Config.EXCLUDE_KEYWORDS)  # ä¼ å…¥ç©ºé›†åˆï¼Œä¸è¿›è¡Œå»é‡
            success = result.success()
            count = len(result.papers)
            results[name] = {"success": success, "count": count, "error": result.error}
            
            if success:
                logger.info(f"âœ… {name} æµ‹è¯•æˆåŠŸï¼Œè·å–åˆ° {count} æ¡ç»“æœ")
                if count > 0:
                    # æ˜¾ç¤ºå‰3æ¡ç»“æœä½œä¸ºç¤ºä¾‹
                    for i, paper in enumerate(result.papers[:3], 1):
                        logger.info(f"  ç¤ºä¾‹ {i}: {paper.title[:60]}...")
            else:
                logger.error(f"âŒ {name} æµ‹è¯•å¤±è´¥: {result.error}")
        except Exception as e:
            logger.error(f"âŒ {name} æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
            results[name] = {"success": False, "count": 0, "error": str(e)}
    
    logger.info("\n" + "=" * 80)
    logger.info("æµ‹è¯•ç»“æœæ±‡æ€»")
    logger.info("=" * 80)
    for name, result in results.items():
        status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"
        logger.info(f"{name:20s} {status:10s} ç»“æœæ•°: {result['count']}")
        if not result["success"] and result.get("error"):
            logger.info(f"{'':20s} {'':10s} é”™è¯¯: {result['error'][:100]}")


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(description="æ™ºèƒ½è®ºæ–‡æ¨é€ç³»ç»Ÿ")
    parser.add_argument('command', choices=['run', 'test-sources'], help='å‘½ä»¤')
    parser.add_argument('--window-days', type=int, help='æŠ“å–çª—å£å¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼‰')
    parser.add_argument('--top-k', type=int, help='é€‰æ‹©Top Kç¯‡ï¼ˆé»˜è®¤5ç¯‡ï¼‰')
    parser.add_argument('--source', type=str, help='æµ‹è¯•å•ä¸ªæ•°æ®æºï¼ˆä»…ç”¨äºtest-sourceså‘½ä»¤ï¼‰ã€‚å¯é€‰å€¼: biorxiv, pubmed, rss, europepmc, sciencenews, github, semanticscholar')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # éªŒè¯é…ç½®
    errors = Config.validate()
    if errors:
        logger.error("é…ç½®é”™è¯¯:")
        for error in errors:
            logger.error(f"  - {error}")
        return
    
    # ä»£ç†å·²åœ¨æ–‡ä»¶å¼€å¤´æ¸…é™¤
    
    if args.command == 'run':
        run_push_task(args.window_days, args.top_k)
    elif args.command == 'test-sources':
        test_sources(args.source)


if __name__ == "__main__":
    main()


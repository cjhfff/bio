"""
AI æŠ¥å‘Šç”Ÿæˆï¼ˆæ”¯æŒTokené¢„ç®—åˆ†é…å’ŒåŠ¨æ€æˆªæ–­ï¼‰
"""
import time
import logging
from typing import List, Tuple
from openai import OpenAI
from backend.models import ScoredPaper, SourceResult
from backend.config import Config
from backend.ranking import get_priority_level

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥tiktokenè¿›è¡Œç²¾ç¡®Tokenè®¡ç®—
try:
    import tiktoken
    HAS_TIKTOKEN = True
    # DeepSeekä½¿ç”¨ç±»ä¼¼GPTçš„tokenizerï¼Œä½¿ç”¨cl100k_baseç¼–ç 
    try:
        _tokenizer = tiktoken.get_encoding("cl100k_base")
    except:
        # å¦‚æœcl100k_baseä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–ç¼–ç 
        try:
            _tokenizer = tiktoken.get_encoding("gpt2")
        except:
            _tokenizer = None
            HAS_TIKTOKEN = False
except ImportError:
    HAS_TIKTOKEN = False
    _tokenizer = None
    logger.warning("tiktokenæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ä¼°ç®—æ–¹æ³•ã€‚å»ºè®®å®‰è£…: pip install tiktoken")


def _generate_fallback_report(scored_papers: List[ScoredPaper], error: Exception) -> str:
    """
    å½“AIè°ƒç”¨å¤±è´¥æ—¶ï¼Œç”Ÿæˆé™çº§æŠ¥å‘Š
    """
    logger.warning("ä½¿ç”¨é™çº§æŠ¥å‘Šç”Ÿæˆæ¨¡å¼ï¼ˆAIè°ƒç”¨å¤±è´¥ï¼‰")
    
    report = "## âš ï¸ æŠ¥å‘Šç”Ÿæˆè¯´æ˜\n\n"
    report += f"ç”±äºAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼ˆé”™è¯¯: {type(error).__name__}: {str(error)}ï¼‰ï¼Œæœ¬æ¬¡ä½¿ç”¨ç®€åŒ–æŠ¥å‘Šæ ¼å¼ã€‚\n\n"
    
    # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
    p0_papers = []
    p1_papers = []
    p2_papers = []
    
    for scored in scored_papers:
        priority = get_priority_level(scored.score)
        if priority == "P0":
            p0_papers.append(scored)
        elif priority == "P1":
            p1_papers.append(scored)
        else:
            p2_papers.append(scored)
    
    # P0 è®ºæ–‡
    if p0_papers:
        report += "## ğŸ”¥ é¡¶çº§è®ºæ–‡ (P0 - æœ€é«˜ä¼˜å…ˆçº§)\n\n"
        for i, scored in enumerate(p0_papers[:5], 1):
            paper = scored.paper
            report += f"**{i}. {paper.title}**\n\n"
            report += f"- æ¥æº: {paper.source}\n"
            report += f"- æ—¥æœŸ: {paper.date}\n"
            if paper.doi:
                report += f"- DOI: {paper.doi}\n"
            if paper.link:
                report += f"- é“¾æ¥: {paper.link}\n"
            if paper.abstract:
                # P0è®ºæ–‡ä¿ç•™å®Œæ•´æ‘˜è¦ï¼ˆæœ€å¤š3000å­—ç¬¦ï¼Œä¸æ­£å¸¸æŠ¥å‘Šä¸€è‡´ï¼‰
                if len(paper.abstract) > 3000:
                    abstract_preview = paper.abstract[:3000] + "\n[æ‘˜è¦è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå»ºè®®æŸ¥çœ‹åŸæ–‡]"
                else:
                    abstract_preview = paper.abstract
                report += f"- æ‘˜è¦: {abstract_preview}\n"
            report += f"- è¯„åˆ†: {scored.score:.1f}\n\n"
    
    # P1 å·¥å…·
    if p1_papers:
        report += "## ğŸ› ï¸ æ–°å·¥å…· (P1 - æŠ€æœ¯å‰å“¨)\n\n"
        for i, scored in enumerate(p1_papers[:3], 1):
            paper = scored.paper
            report += f"**{i}. {paper.title}**\n\n"
            if paper.link:
                report += f"- é“¾æ¥: {paper.link}\n"
            report += f"- æ¥æº: {paper.source}\n"
            report += f"- æ—¥æœŸ: {paper.date}\n"
            if paper.abstract:
                # P1è®ºæ–‡ä¿ç•™å‰800å­—ç¬¦ï¼ˆæ¯”P0å°‘ï¼Œä½†æ¯”åŸæ¥çš„300å¤šï¼‰
                if len(paper.abstract) > 800:
                    abstract_preview = paper.abstract[:800] + "..."
                else:
                    abstract_preview = paper.abstract
                report += f"- æ‘˜è¦: {abstract_preview}\n"
            report += "\n"
    
    # P2 å…³è”æ€§æŒ–æ˜
    if p2_papers and not p0_papers:
        report += "## ğŸ’¡ å…³è”æ€§æŒ–æ˜ (P2 - å¯å‘ä»·å€¼)\n\n"
        report += "æœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æœªå‘ç°ç›´æ¥å±äºä¸‰å¤§æ–¹å‘çš„æ ¸å¿ƒæ›´æ–°ï¼Œä½†å‘ç°ä»¥ä¸‹å¯èƒ½ç›¸å…³çš„è®ºæ–‡ï¼š\n\n"
        for i, scored in enumerate(p2_papers[:3], 1):
            paper = scored.paper
            report += f"**{i}. {paper.title}**\n\n"
            report += f"- æ¥æº: {paper.source}\n"
            if paper.link:
                report += f"- é“¾æ¥: {paper.link}\n"
            if paper.abstract:
                # P2è®ºæ–‡ä¿ç•™å‰500å­—ç¬¦
                if len(paper.abstract) > 500:
                    abstract_preview = paper.abstract[:500] + "..."
                else:
                    abstract_preview = paper.abstract
                report += f"- æ‘˜è¦: {abstract_preview}\n"
            report += "\n"
    
    # æ•°æ®ç»Ÿè®¡
    report += "## ğŸ“Š æ•°æ®ç»Ÿè®¡\n\n"
    report += f"- æœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æ–°å¢è®ºæ–‡æ•°: {len(scored_papers)}\n"
    report += f"- ä¼˜å…ˆçº§åˆ†å¸ƒ: P0={len(p0_papers)}ç¯‡, P1={len(p1_papers)}ç¯‡, P2={len(p2_papers)}ç¯‡\n\n"
    
    report += "---\n\n"
    report += "*æ³¨ï¼šç”±äºAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œæœ¬æŠ¥å‘Šä¸ºç®€åŒ–ç‰ˆæœ¬ã€‚å»ºè®®ç¨åé‡æ–°è¿è¡Œä»¥è·å–å®Œæ•´AIåˆ†ææŠ¥å‘Šã€‚*\n"
    
    return report


def estimate_tokens(text: str) -> int:
    """
    ç²¾ç¡®è®¡ç®—æ–‡æœ¬çš„Tokenæ•°ï¼ˆä¼˜å…ˆä½¿ç”¨tiktokenï¼Œå¦åˆ™ä½¿ç”¨ä¿å®ˆä¼°ç®—ï¼‰
    
    è€ƒè™‘åˆ°ç”ŸåŒ–é¢†åŸŸçš„å¤æ‚æœ¯è¯­ï¼ˆå¦‚Phosphatidylethanolamineã€æ ¹ç˜¤èŒå…±ç”Ÿä½“ç­‰ï¼‰ï¼Œ
    ä½¿ç”¨ç²¾ç¡®çš„tokenizerå¯ä»¥é¿å…éçº¿æ€§é£é™©ã€‚
    
    Args:
        text: å¾…è®¡ç®—çš„æ–‡æœ¬
        
    Returns:
        Tokenæ•°é‡ï¼ˆåŒ…å«10%å®‰å…¨å†—ä½™ï¼‰
    """
    if not text:
        return 0
    
    # ä¼˜å…ˆä½¿ç”¨tiktokenè¿›è¡Œç²¾ç¡®è®¡ç®—
    if HAS_TIKTOKEN and _tokenizer:
        try:
            tokens = _tokenizer.encode(text)
            # æ·»åŠ 10%å®‰å…¨å†—ä½™ï¼ˆè€ƒè™‘åˆ°promptä¸­çš„æŒ‡ä»¤ç­‰ï¼‰
            return int(len(tokens) * 1.1)
        except Exception as e:
            logger.warning(f"tiktokenè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•: {e}")
    
    # é™çº§æ–¹æ¡ˆï¼šä¿å®ˆä¼°ç®—ï¼ˆé’ˆå¯¹ä¸­è‹±æ–‡æ··åˆ+ç”ŸåŒ–æœ¯è¯­ï¼‰
    # è€ƒè™‘åˆ°å¤æ‚ç”ŸåŒ–æœ¯è¯­çš„subwordåˆ‡åˆ†ï¼Œä½¿ç”¨1.0-1.2å€ç‡
    # ä¸­æ–‡å­—ç¬¦çº¦0.6-1.5 tokenï¼Œè‹±æ–‡å•è¯çº¦0.25-1.5 tokenï¼ˆå–å†³äºé•¿åº¦ï¼‰
    # æ··åˆæ–‡æœ¬ä½¿ç”¨1.0å€ç‡ï¼Œç„¶åæ·»åŠ 20%å†—ä½™
    return int(len(text) * 1.0 * 1.2)


def prepare_papers_for_llm(scored_papers: List[ScoredPaper], max_tokens: int = None, batch_size: int = None) -> Tuple[str, int, int]:
    """
    ä¸º LLM å‡†å¤‡è®ºæ–‡è¾“å…¥ï¼ˆæ”¯æŒå•ç¯‡ç²¾è¯»æ¨¡å¼å’Œæ‰¹é‡æ¨¡å¼ï¼‰
    
    Args:
        scored_papers: å¸¦è¯„åˆ†çš„è®ºæ–‡åˆ—è¡¨
        max_tokens: æœ€å¤§Tokené™åˆ¶,é»˜è®¤ä½¿ç”¨Config.LLM_MAX_CONTEXT_TOKENS
        batch_size: æ‰¹æ¬¡å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰è®ºæ–‡ï¼Œå¦‚æœæŒ‡å®šåˆ™åªå¤„ç†å‰batch_sizeç¯‡
    
    Returns:
        (papers_text, åŒ…å«çš„è®ºæ–‡æ•°, ä¼°ç®—Tokenæ•°)
    """
    if max_tokens is None:
        max_tokens = Config.LLM_MAX_CONTEXT_TOKENS
    
    if len(scored_papers) == 0:
        return "", 0, 0
    
    # å¦‚æœæŒ‡å®šäº†batch_sizeï¼Œåªå¤„ç†å‰batch_sizeç¯‡
    papers_to_process = scored_papers[:batch_size] if batch_size else scored_papers
    
    paper_blocks = []
    total_tokens = 0
    
    for idx, scored in enumerate(papers_to_process, 1):
        paper = scored.paper
        priority = get_priority_level(scored.score)
        
        # æ‰¹é‡æ¨¡å¼ï¼šä¿ç•™å®Œæ•´æ‘˜è¦ï¼ˆä¸æˆªæ–­ï¼‰
        abstract_text = paper.abstract if paper.abstract else ""
        # ä»…åœ¨æç«¯æƒ…å†µä¸‹ï¼ˆ>5000å­—ç¬¦ï¼‰æ‰æˆªæ–­
        if len(abstract_text) > 5000:
            abstract_text = abstract_text[:5000] + "\n[æ‘˜è¦è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå»ºè®®æŸ¥çœ‹åŸæ–‡]"
        
        # æ„å»ºè®ºæ–‡æ–‡æœ¬å—ï¼ˆå®Œæ•´ç»“æ„åŒ–ä¿¡æ¯ï¼‰
        paper_block = f"ã€è®ºæ–‡ {idx}ã€‘\n"
        paper_block += f"æ ‡é¢˜: {paper.title}\n\n"
        paper_block += f"æœŸåˆŠ/æ¥æº: {paper.source}\n\n"
        paper_block += f"å‘å¸ƒæ—¶é—´: {paper.date}\n\n"
        paper_block += f"æ‘˜è¦: {abstract_text}\n\n"
        
        # æ·»åŠ è¯„åˆ†ä¿¡æ¯ï¼ˆè®©AIçŸ¥é“ä¸ºä»€ä¹ˆè¿™ç¯‡è®ºæ–‡è¢«é€‰ä¸­ï¼‰
        paper_block += f"è¯„åˆ†: {scored.score:.1f}åˆ† (ä¼˜å…ˆçº§: {priority})\n"
        if scored.reasons:
            # æ·»åŠ æ‰€æœ‰è¯„åˆ†ç†ç”±ï¼ˆå¸®åŠ©AIç†è§£è®ºæ–‡ä¸ä¸‰å¤§æ–¹å‘çš„å…³è”ï¼‰
            reason_summary = "; ".join([r.description for r in scored.reasons])
            paper_block += f"è¯„åˆ†ç†ç”±: {reason_summary}\n"
        paper_block += "\n"
        
        if paper.doi:
            paper_block += f"DOI: {paper.doi}\n\n"
        if paper.link:
            paper_block += f"é“¾æ¥: {paper.link}\n\n"
        if paper.citation_count > 0:
            paper_block += f"å¼•ç”¨æ•°: {paper.citation_count}\n"
            if paper.influential_count > 0:
                paper_block += f"é«˜å½±å“åŠ›å¼•ç”¨: {paper.influential_count}\n"
        paper_block += "---\n\n"
        
        # ä¼°ç®—Tokenæ•°
        block_tokens = estimate_tokens(paper_block)
        total_tokens += block_tokens
        paper_blocks.append(paper_block)
    
    papers_text = "".join(paper_blocks)
    
    mode_name = f"æ‰¹é‡æ¨¡å¼ï¼ˆ{len(papers_to_process)}ç¯‡ï¼‰" if batch_size else "å•ç¯‡ç²¾è¯»æ¨¡å¼"
    logger.info(
        f"[{mode_name}] åŒ…å« {len(papers_to_process)} ç¯‡è®ºæ–‡, ä¼°ç®—Tokenæ•°: {total_tokens}/{max_tokens}"
    )
    
    return papers_text, len(papers_to_process), total_tokens


def generate_single_paper_report(scored_paper: ScoredPaper, paper_num: int) -> str:
    """
    ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœæ€»ç»“
    
    Args:
        scored_paper: å¸¦è¯„åˆ†çš„è®ºæ–‡
        paper_num: è®ºæ–‡ç¼–å·
        
    Returns:
        ç”Ÿæˆçš„è®ºæ–‡æŠ¥å‘Šæ–‡æœ¬
    """
    if not scored_paper:
        return ""
    
    # å‡†å¤‡è®ºæ–‡æ–‡æœ¬ï¼ˆå•ç¯‡ï¼ŒåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
    papers_text, _, _ = prepare_papers_for_llm([scored_paper], batch_size=1)
    
    # æ„å»ºæç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼Œåªæ€»ç»“é‡è¦ç ”ç©¶æˆæœï¼‰
    prompt = f"""ä½ æ˜¯ä¸€åä¸“æ³¨äºç”Ÿç‰©åŒ–å­¦ä¸åˆ†å­ç”Ÿç‰©å­¦ç ”ç©¶çš„ä¸“å®¶ã€‚è¯·æ€»ç»“ä»¥ä¸‹è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœã€‚

**é‡è¦ï¼šæ£€ç´¢èŒƒå›´ä»…é™ä»¥ä¸‹ä¸‰ä¸ªç ”ç©¶æ–¹å‘ï¼Œå…¶ä»–æ–¹å‘çš„ç ”ç©¶è¯·å¿½ç•¥ï¼š**
1. ç”Ÿç‰©å›ºæ°®ï¼ˆBiological Nitrogen Fixationï¼‰
2. èƒå¤–ä¿¡å·æ„ŸçŸ¥ä¸ä¼ é€’ï¼ˆExtracellular Signal Perception and Transductionï¼ŒåŒ…å«ç»†èƒè†œè¡¨é¢å—ä½“/PRR/RLK ä»‹å¯¼çš„ PTI ç­‰æ¤ç‰©å…ç–«ä¿¡å·ï¼‰
3. é…¶çš„ç»“æ„ä¸ä½œç”¨æœºåˆ¶ï¼ˆEnzyme Structure and Mechanismï¼‰

**ä»»åŠ¡è¦æ±‚ï¼š**
è¯·ç®€æ´åœ°æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

### ã€è®ºæ–‡æ ‡é¢˜ã€‘ (æœŸåˆŠ: XXX, å‘å¸ƒæ—¥æœŸ: YYYY-MM-DD)

**é‡è¦ç ”ç©¶æˆæœï¼š**
- æ ¸å¿ƒç§‘å­¦å‘ç°ï¼ˆ1-2å¥è¯æ¦‚æ‹¬ï¼‰
- åˆ›æ–°ç‚¹å’Œçªç ´æ€§æˆæœï¼ˆ1-2å¥è¯ï¼‰
- å¯¹ä¸‰å¤§ç ”ç©¶æ–¹å‘çš„é‡è¦æ„ä¹‰ï¼ˆ1å¥è¯ï¼‰

**æŠ€æœ¯æ–¹æ³•ï¼š**ï¼ˆå¦‚æœ‰ç»“æ„ç”Ÿç‰©å­¦æ–¹æ³•æˆ–æ–°æŠ€æœ¯ï¼Œç®€è¦è¯´æ˜ï¼‰

**å‚è€ƒæ–‡çŒ®ï¼š**
- DOI: [å¦‚æœ‰]
- é“¾æ¥: [å¦‚æœ‰]

**è¦æ±‚ï¼š**
- æ€»å­—æ•°æ§åˆ¶åœ¨200-400å­—
- è¯­è¨€ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
- å¿…é¡»æ ‡æ³¨å‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šå‘å¸ƒæ—¥æœŸ: YYYY-MM-DDï¼‰
- **é‡è¦ï¼šå¦‚æœè®ºæ–‡ä¸å±äºä¸‰å¤§ç ”ç©¶æ–¹å‘ï¼ˆç”Ÿç‰©å›ºæ°®ã€èƒå¤–ä¿¡å·æ„ŸçŸ¥ä¸ä¼ é€’ã€é…¶çš„ç»“æ„ä¸ä½œç”¨æœºåˆ¶ï¼‰ï¼Œè¯·è¿”å›ä»¥ä¸‹æ ¼å¼ï¼š**
  ```
  ## ä¸ç›¸å…³è®ºæ–‡
  
  **è®ºæ–‡æ ‡é¢˜ï¼š** [è®ºæ–‡æ ‡é¢˜]
  
  **è¿‡æ»¤åŸå› ï¼š** [ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆä¸å±äºä¸‰å¤§ç ”ç©¶æ–¹å‘ï¼Œä¾‹å¦‚ï¼šè¯¥ç ”ç©¶ä¸»è¦å…³æ³¨XXXé¢†åŸŸï¼Œä¸ç”Ÿç‰©å›ºæ°®ã€èƒå¤–ä¿¡å·æ„ŸçŸ¥ä¸ä¼ é€’ã€é…¶çš„ç»“æ„ä¸ä½œç”¨æœºåˆ¶æ— ç›´æ¥å…³è”]
  
  æ­¤è®ºæ–‡ä¸å±äºæŒ‡å®šçš„ä¸‰å¤§ç ”ç©¶æ–¹å‘ï¼Œå·²è‡ªåŠ¨è¿‡æ»¤ã€‚
  ```
  å¦‚æœè®ºæ–‡å±äºä¸‰å¤§ç ”ç©¶æ–¹å‘ï¼Œè¯·æŒ‰æ­£å¸¸æ ¼å¼è¿”å›ã€‚

{papers_text}
"""
    
    # è·å–æ‰€æœ‰ API å¯†é’¥ï¼ˆä¸»å¯†é’¥ + å¤‡ç”¨å¯†é’¥ï¼‰
    all_api_keys = Config.get_all_api_keys()
    logger.info(f"[è®ºæ–‡ {paper_num}] å¼€å§‹ç”ŸæˆæŠ¥å‘Šï¼Œå¯ç”¨ API å¯†é’¥æ•°é‡: {len(all_api_keys)}")
    
    # éå†æ‰€æœ‰ API å¯†é’¥ï¼Œå¦‚æœå½“å‰å¯†é’¥å¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª
    last_error = None
    for key_index, api_key in enumerate(all_api_keys):
        key_name = "ä¸»å¯†é’¥" if key_index == 0 else f"å¤‡ç”¨å¯†é’¥{key_index}"
        masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "***"
        logger.info(f"[è®ºæ–‡ {paper_num}] å°è¯•ä½¿ç”¨ {key_name}: {masked_key}")
        
        # æ¯ä¸ªå¯†é’¥çš„é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰
        max_retries_per_key = Config.API_MAX_RETRIES
        for attempt in range(max_retries_per_key):
            try:
                logger.info(f"[è®ºæ–‡ {paper_num}] æ­£åœ¨è°ƒç”¨AIç”ŸæˆæŠ¥å‘Šï¼ˆ{key_name}, ç¬¬ {attempt + 1}/{max_retries_per_key} æ¬¡å°è¯•ï¼‰...")
                
                # æ¯æ¬¡é‡è¯•éƒ½åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹ï¼Œé¿å…è¿æ¥å¤ç”¨é—®é¢˜
                client = OpenAI(
                    api_key=api_key,
                    base_url=Config.DEEPSEEK_BASE_URL,
                    timeout=Config.API_TIMEOUT,
                    max_retries=0  # ç¦ç”¨SDKå†…ç½®é‡è¯•ï¼Œæ‰‹åŠ¨æ§åˆ¶
                )
                
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç”Ÿç‰©åŒ–å­¦ä¸åˆ†å­ç”Ÿç‰©å­¦é¢†åŸŸçš„ä¸“å®¶ï¼Œæ“…é•¿ç®€æ´åœ°æ€»ç»“å­¦æœ¯è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=1000  # å•ç¯‡æ€»ç»“ï¼Œå‡å°‘tokens
                )
                
                paper_report = response.choices[0].message.content
                logger.info(f"âœ… [è®ºæ–‡ {paper_num}] AIæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼(ä½¿ç”¨ {key_name}, è€—æ—¶çº¦ {attempt + 1} æ¬¡å°è¯•)")
                return paper_report
                
            except KeyboardInterrupt:
                logger.warning("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­äº†ç¨‹åº")
                raise
            except Exception as e:
                last_error = e
                error_type = type(e).__name__
                error_msg = str(e)
                logger.error(f"[è®ºæ–‡ {paper_num}] è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹æ—¶å‡ºé”™ï¼ˆ{key_name}, ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰: {error_type}: {error_msg[:200]}")
                
                # å¦‚æœæ˜¯è®¤è¯é”™è¯¯ï¼ˆ401ï¼‰ï¼Œç›´æ¥åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥
                if "401" in error_msg or "Unauthorized" in error_msg or "invalid" in error_msg.lower():
                    logger.warning(f"[è®ºæ–‡ {paper_num}] {key_name} è®¤è¯å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥")
                    break
                
                # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥é‡è¯•
                if attempt < max_retries_per_key - 1:
                    # æŒ‡æ•°é€€é¿ï¼š10ç§’ã€20ç§’ã€40ç§’ï¼Œä½†ä¸è¶…è¿‡æœ€å¤§å»¶è¿Ÿ
                    wait_time = min(Config.API_RETRY_BASE_DELAY * (2 ** attempt), Config.API_RETRY_MAX_DELAY)
                    logger.info(f"[è®ºæ–‡ {paper_num}] ç­‰å¾… {wait_time} ç§’åé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰...")
                    time.sleep(wait_time)
                else:
                    logger.warning(f"[è®ºæ–‡ {paper_num}] {key_name} æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥")
                    if key_index < len(all_api_keys) - 1:
                        time.sleep(5)  # åˆ‡æ¢å¯†é’¥å‰ç­‰å¾…æ›´é•¿æ—¶é—´
                    break
    
    # æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥ï¼Œè¿”å›é™çº§æŠ¥å‘Š
    logger.error(f"[è®ºæ–‡ {paper_num}] æ‰€æœ‰ API å¯†é’¥ï¼ˆå…± {len(all_api_keys)} ä¸ªï¼‰å‡å¤±è´¥ï¼Œè¿”å›é™çº§æŠ¥å‘Š")
    return _generate_fallback_report([scored_paper], last_error)


def generate_final_summary(all_paper_reports: List[str], total_papers: int, source_results: List[SourceResult] = None) -> str:
    """
    ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Šï¼ˆä¿ç•™æ‰€æœ‰è®ºæ–‡é‡è¦ç ”ç©¶æˆæœ + æ€»ä½“æ€»ç»“ï¼‰
    
    Args:
        all_paper_reports: æ‰€æœ‰è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœæŠ¥å‘Šåˆ—è¡¨
        total_papers: æ€»è®ºæ–‡æ•°
        source_results: æ•°æ®æºæŠ½å–ç»“æœåˆ—è¡¨(å¯é€‰,ç”¨äºç»Ÿè®¡)
        
    Returns:
        ç”Ÿæˆçš„æœ€ç»ˆç»¼åˆæŠ¥å‘Šæ–‡æœ¬
    """
    if not all_paper_reports:
        return "æœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æœªå‘ç°ç›¸å…³è®ºæ–‡ã€‚"
    
    # åˆå¹¶æ‰€æœ‰è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœ
    detailed_content = "\n\n".join([f"## è®ºæ–‡ {i+1} é‡è¦ç ”ç©¶æˆæœ\n\n{report}" for i, report in enumerate(all_paper_reports)])
    
    # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œå¦‚æœè¶…è¿‡é˜ˆå€¼ï¼ˆ20000å­—ç¬¦ï¼‰ï¼Œåªä¼ é€’å‰Nç¯‡è®ºæ–‡ç»™API
    MAX_CONTENT_LENGTH = 20000
    reports_to_use = all_paper_reports
    if len(detailed_content) > MAX_CONTENT_LENGTH:
        # è®¡ç®—å¯ä»¥åŒ…å«å¤šå°‘ç¯‡è®ºæ–‡
        current_length = 0
        reports_to_use = []
        for i, report in enumerate(all_paper_reports):
            report_with_header = f"## è®ºæ–‡ {i+1} é‡è¦ç ”ç©¶æˆæœ\n\n{report}\n\n"
            if current_length + len(report_with_header) > MAX_CONTENT_LENGTH:
                break
            reports_to_use.append(report)
            current_length += len(report_with_header)
        
        logger.warning(f"è®ºæ–‡æŠ¥å‘Šæ€»é•¿åº¦({len(detailed_content)}å­—ç¬¦)è¶…è¿‡é˜ˆå€¼({MAX_CONTENT_LENGTH}å­—ç¬¦)ï¼Œä»…ä½¿ç”¨å‰{len(reports_to_use)}ç¯‡è®ºæ–‡ç”Ÿæˆæœ€ç»ˆæ€»ç»“")
        detailed_content = "\n\n".join([f"## è®ºæ–‡ {i+1} é‡è¦ç ”ç©¶æˆæœ\n\n{report}" for i, report in enumerate(reports_to_use)])
    
    # æ„å»ºæ•°æ®æºç»Ÿè®¡ä¿¡æ¯
    source_stats = ""
    if source_results:
        total_from_all_sources = sum(len(r.papers) for r in source_results)
        source_stats = "\n<metadata>\n"
        source_stats += "**æ•°æ®æºç»Ÿè®¡ä¿¡æ¯ï¼ˆå…ƒæ•°æ®ï¼Œä»…ä¾›äº†è§£æœç´¢è¦†ç›–å¹¿åº¦ï¼‰ï¼š**\n"
        for r in source_results:
            source_stats += f"- {r.source_name}: {len(r.papers)} æ¡\n"
        source_stats += f"- æ€»è®¡: {total_from_all_sources} æ¡ï¼ˆæ‰€æœ‰æ•°æ®æºï¼‰\n"
        source_stats += f"- ç»è¿‡è¯„åˆ†ç­›é€‰åï¼Œå…± {total_papers} ç¯‡è¿›å…¥æœ€ç»ˆåˆ†æ\n"
        if len(reports_to_use) < len(all_paper_reports):
            source_stats += f"- æ³¨æ„ï¼šç”±äºå†…å®¹è¿‡é•¿ï¼Œæœ€ç»ˆæ€»ç»“ä»…åŸºäºå‰ {len(reports_to_use)} ç¯‡è®ºæ–‡\n"
        source_stats += "</metadata>\n\n"
    
    # æ„å»ºæœ€ç»ˆæ€»ç»“çš„æç¤ºè¯
    prompt = f"""ä½ æ˜¯ä¸€åä¸“æ³¨äºç”Ÿç‰©åŒ–å­¦ä¸åˆ†å­ç”Ÿç‰©å­¦ç ”ç©¶çš„ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯ä»Šå¤©æ£€ç´¢åˆ°çš„æ‰€æœ‰è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœæ€»ç»“ã€‚

{source_stats}

**é‡è¦ï¼šæ£€ç´¢èŒƒå›´ä»…é™ä»¥ä¸‹ä¸‰ä¸ªç ”ç©¶æ–¹å‘ï¼š**
1. ç”Ÿç‰©å›ºæ°®ï¼ˆBiological Nitrogen Fixationï¼‰
2. èƒå¤–ä¿¡å·æ„ŸçŸ¥ä¸ä¼ é€’ï¼ˆExtracellular Signal Perception and Transductionï¼ŒåŒ…å«ç»†èƒè†œè¡¨é¢å—ä½“/PRR/RLK ä»‹å¯¼çš„ PTI ç­‰æ¤ç‰©å…ç–«ä¿¡å·ï¼‰
3. é…¶çš„ç»“æ„ä¸ä½œç”¨æœºåˆ¶ï¼ˆEnzyme Structure and Mechanismï¼‰

**ä»»åŠ¡è¦æ±‚ï¼š**
1. **ä¿ç•™æ‰€æœ‰è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœ**ï¼šå®Œæ•´ä¿ç•™ä¸‹æ–¹åˆ—å‡ºçš„æ‰€æœ‰è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœå†…å®¹
2. **æ·»åŠ æ€»ä½“æ€»ç»“**ï¼šåœ¨ä¿ç•™è¯¦ç»†å†…å®¹çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ ä»¥ä¸‹æ€»ä½“åˆ†æï¼š
   - ä»Šæ—¥ç ”ç©¶çƒ­ç‚¹æ€»ç»“ï¼šæ¦‚æ‹¬ä»Šå¤©æ‰€æœ‰è®ºæ–‡åæ˜ çš„ç ”ç©¶è¶‹åŠ¿
   - è·¨è®ºæ–‡å…³è”åˆ†æï¼šåˆ†æä¸åŒè®ºæ–‡ä¹‹é—´çš„å…³è”æ€§å’ŒååŒä»·å€¼
   - é¢†åŸŸå‘å±•åŠ¨æ€ï¼šæ€»ç»“å¯¹ä¸‰å¤§ç ”ç©¶æ–¹å‘çš„æ•´ä½“æ¨åŠ¨ä½œç”¨
   - é‡è¦å‘ç°äº®ç‚¹ï¼šæç‚¼ä»Šå¤©æœ€é‡è¦çš„ç§‘å­¦å‘ç°å’Œçªç ´

**æŠ¥å‘Šç»“æ„ï¼š**
1. é¦–å…ˆæ·»åŠ "## ğŸ“Š ä»Šæ—¥ç ”ç©¶æ€»ç»“"éƒ¨åˆ†ï¼ˆæ€»ä½“åˆ†æï¼Œ300-500å­—ï¼‰
2. ç„¶åå®Œæ•´ä¿ç•™æ‰€æœ‰è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœå†…å®¹
3. æœ€åæ·»åŠ "## ğŸ“ˆ æ•°æ®ç»Ÿè®¡"éƒ¨åˆ†

è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ï¼Œç”Ÿæˆä¸€ä»½ç»¼åˆæ€§çš„æ¯æ—¥æƒ…æŠ¥å†…å‚ã€‚

ä»¥ä¸‹æ˜¯æ‰€æœ‰è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœï¼š

{detailed_content}
"""
    
    # è·å–æ‰€æœ‰ API å¯†é’¥
    all_api_keys = Config.get_all_api_keys()
    logger.info(f"å¼€å§‹ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Šï¼Œå¯ç”¨ API å¯†é’¥æ•°é‡: {len(all_api_keys)}")
    
    # éå†æ‰€æœ‰ API å¯†é’¥
    last_error = None
    for key_index, api_key in enumerate(all_api_keys):
        key_name = "ä¸»å¯†é’¥" if key_index == 0 else f"å¤‡ç”¨å¯†é’¥{key_index}"
        masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "***"
        logger.info(f"å°è¯•ä½¿ç”¨ {key_name}: {masked_key}")
        
        max_retries_per_key = Config.API_MAX_RETRIES
        for attempt in range(max_retries_per_key):
            try:
                logger.info(f"æ­£åœ¨è°ƒç”¨AIç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Šï¼ˆ{key_name}, ç¬¬ {attempt + 1}/{max_retries_per_key} æ¬¡å°è¯•ï¼‰...")
                
                # æ¯æ¬¡é‡è¯•éƒ½åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹ï¼Œé¿å…è¿æ¥å¤ç”¨é—®é¢˜
                client = OpenAI(
                    api_key=api_key,
                    base_url=Config.DEEPSEEK_BASE_URL,
                    timeout=Config.API_TIMEOUT,
                    max_retries=0  # ç¦ç”¨SDKå†…ç½®é‡è¯•ï¼Œæ‰‹åŠ¨æ§åˆ¶
                )
                
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç”Ÿç‰©åŒ–å­¦ä¸åˆ†å­ç”Ÿç‰©å­¦é¢†åŸŸçš„ä¸“å®¶ï¼Œæ“…é•¿ç»¼åˆåˆ†æå¤šç¯‡è®ºæ–‡ï¼Œæç‚¼ç ”ç©¶è¶‹åŠ¿å’Œé‡è¦å‘ç°ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=4000
                )
                
                final_report = response.choices[0].message.content
                logger.info(f"âœ… æœ€ç»ˆæ€»ç»“æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼(ä½¿ç”¨ {key_name})")
                
                # æ·»åŠ æ•°æ®å®Œæ•´æ€§æç¤º
                if source_results:
                    degraded_sources = [sr for sr in source_results if sr.is_degraded or sr.error]
                    if degraded_sources:
                        final_report += "\n\n## âš ï¸ æ•°æ®å®Œæ•´æ€§è¯´æ˜\n\n"
                        final_report += "æœ¬æ¬¡æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­,ä»¥ä¸‹æ•°æ®æºæœªèƒ½å®Œæ•´è·å–:\n"
                        for sr in degraded_sources:
                            if sr.error:
                                final_report += f"- {sr.source_name}: æŠ½å–å¤±è´¥ ({sr.error})\n"
                            elif sr.degraded_reason:
                                final_report += f"- {sr.source_name}: {sr.degraded_reason}\n"
                        final_report += "\nå»ºè®®å…³æ³¨åç»­æ›´æ–°,æˆ–æ‰‹åŠ¨è®¿é—®å¯¹åº”æ•°æ®æºç¡®è®¤ã€‚\n"
                
                return final_report
                
            except KeyboardInterrupt:
                logger.warning("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­äº†ç¨‹åº")
                raise
            except Exception as e:
                last_error = e
                error_type = type(e).__name__
                error_msg = str(e)
                logger.error(f"è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹æ—¶å‡ºé”™ï¼ˆ{key_name}, ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰: {error_type}: {error_msg[:200]}")
                
                if "401" in error_msg or "Unauthorized" in error_msg or "invalid" in error_msg.lower():
                    logger.warning(f"{key_name} è®¤è¯å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥")
                    break
                
                if attempt < max_retries_per_key - 1:
                    # æŒ‡æ•°é€€é¿ç­–ç•¥
                    wait_time = min(Config.API_RETRY_BASE_DELAY * (2 ** attempt), Config.API_RETRY_MAX_DELAY)
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰...")
                    time.sleep(wait_time)
                else:
                    logger.warning(f"{key_name} æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥")
                    if key_index < len(all_api_keys) - 1:
                        time.sleep(2)
                    break
    
    # æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥ï¼Œç”ŸæˆåŸºäºå…³é”®è¯çš„ç®€å•ç»Ÿè®¡æ€»ç»“
    logger.error(f"æ‰€æœ‰ API å¯†é’¥ï¼ˆå…± {len(all_api_keys)} ä¸ªï¼‰å‡å¤±è´¥ï¼Œç”ŸæˆåŸºäºå…³é”®è¯çš„ç®€å•ç»Ÿè®¡æ€»ç»“")
    
    # æå–å…³é”®è¯ç»Ÿè®¡
    from collections import Counter
    keywords_found = []
    for report in all_paper_reports:
        # ç®€å•æå–å¯èƒ½çš„å…³é”®è¯ï¼ˆä»æŠ¥å‘Šæ ‡é¢˜å’Œå†…å®¹ä¸­ï¼‰
        if "ç»“æ„" in report or "structure" in report.lower() or "cryo-em" in report.lower():
            keywords_found.append("ç»“æ„ç”Ÿç‰©å­¦")
        if "å›ºæ°®" in report or "nitrogen" in report.lower():
            keywords_found.append("ç”Ÿç‰©å›ºæ°®")
        if "ä¿¡å·" in report or "signal" in report.lower() or "å—ä½“" in report:
            keywords_found.append("ä¿¡å·è½¬å¯¼")
        if "é…¶" in report or "enzyme" in report.lower():
            keywords_found.append("é…¶æœºåˆ¶")
    
    keyword_counts = Counter(keywords_found)
    keyword_summary = "ã€".join([f"{kw}({count}ç¯‡)" for kw, count in keyword_counts.most_common(5)])
    
    # ç”Ÿæˆç®€å•ç»Ÿè®¡æ€»ç»“
    simple_summary = f"""## ğŸ“Š ä»Šæ—¥ç ”ç©¶æ€»ç»“

**ç ”ç©¶çƒ­ç‚¹ï¼š** {keyword_summary if keyword_summary else "æœªæ£€æµ‹åˆ°æ˜æ˜¾çƒ­ç‚¹"}

**è®ºæ–‡åˆ†å¸ƒï¼š** å…±æ£€ç´¢åˆ° {total_papers} ç¯‡ç›¸å…³è®ºæ–‡ï¼Œä¸»è¦æ¶‰åŠç»“æ„ç”Ÿç‰©å­¦ã€é…¶æœºåˆ¶ã€ä¿¡å·è½¬å¯¼ç­‰æ–¹å‘ã€‚

**é‡è¦æç¤ºï¼š** ç”±äºAPIè¿æ¥é—®é¢˜ï¼Œæœªèƒ½ç”Ÿæˆè¯¦ç»†çš„ç»¼åˆåˆ†æã€‚ä»¥ä¸‹æ˜¯æ‰€æœ‰è®ºæ–‡çš„é‡è¦ç ”ç©¶æˆæœæ€»ç»“ï¼š

{detailed_content}

*æ³¨ï¼šå•ç¯‡è®ºæ–‡åˆ†æå·²å®Œæˆï¼Œæœ€ç»ˆæ€»ç»“æŠ¥å‘Šå› APIè¿æ¥é—®é¢˜æœªç”Ÿæˆï¼Œå»ºè®®ç¨åé‡æ–°è¿è¡Œä»¥è·å–å®Œæ•´çš„AIç»¼åˆåˆ†ææŠ¥å‘Šã€‚*
"""
    
    return simple_summary


def generate_daily_report(scored_papers: List[ScoredPaper], source_results: List[SourceResult] = None) -> str:
    """
    ä½¿ç”¨ AI ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š(æ”¯æŒTokené¢„ç®—å’Œæ•°æ®å®Œæ•´æ€§æç¤º)
    
    Args:
        scored_papers: å¸¦è¯„åˆ†çš„è®ºæ–‡åˆ—è¡¨
        source_results: æ•°æ®æºæŠ½å–ç»“æœåˆ—è¡¨(å¯é€‰,ç”¨äºé™çº§æ£€æµ‹)
        
    Returns:
        ç”Ÿæˆçš„æŠ¥å‘Šæ–‡æœ¬
    """
    if not scored_papers:
        return "æœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æœªå‘ç°ç›´æ¥å±äºä¸‰å¤§ç ”ç©¶æ–¹å‘ï¼ˆç”Ÿç‰©å›ºæ°®ã€èƒå¤–ä¿¡å·æ„ŸçŸ¥ä¸ä¼ é€’ã€é…¶çš„ç»“æ„ä¸ä½œç”¨æœºåˆ¶ï¼‰çš„æ ¸å¿ƒæ›´æ–°ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š1) æ£€ç´¢æ—¶é—´çª—å£å†…ç¡®å®æ²¡æœ‰ç›¸å…³æ–°è®ºæ–‡ï¼›2) æ•°æ®æºæŠ“å–å­˜åœ¨é—®é¢˜ã€‚å»ºè®®æ£€æŸ¥æ•°æ®æºè¿æ¥æˆ–æ‰©å¤§æ£€ç´¢èŒƒå›´ã€‚"
    
    # ä½¿ç”¨åŠ¨æ€ä¸Šä¸‹æ–‡ç®¡ç†æˆ–ä¼ ç»Ÿå…¨é‡æ‹¼æ¥
    if Config.ENABLE_DYNAMIC_CONTEXT_MANAGEMENT:
        papers_text, included_count, token_count = prepare_papers_for_llm(scored_papers)
        logger.info(f"[åŠ¨æ€ä¸Šä¸‹æ–‡] å·²å¯ç”¨, åŒ…å« {included_count}/{len(scored_papers)} ç¯‡è®ºæ–‡")
    else:
        # ä¼ ç»Ÿé€»è¾‘(å‘åå…¼å®¹)
        papers_text = ""
        for i, scored in enumerate(scored_papers):
            paper = scored.paper
            papers_text += f"ã€è®ºæ–‡ {i+1}ã€‘\n"
            papers_text += f"æ ‡é¢˜: {paper.title}\n\n"
            papers_text += f"æœŸåˆŠ/æ¥æº: {paper.source}\n\n"
            papers_text += f"å‘å¸ƒæ—¶é—´: {paper.date}\n\n"
            papers_text += f"æ‘˜è¦: {paper.abstract}\n\n"
            
            if paper.doi:
                papers_text += f"DOI: {paper.doi}\n\n"
            if paper.link:
                papers_text += f"é“¾æ¥: {paper.link}\n\n"
            if paper.citation_count > 0:
                papers_text += f"å¼•ç”¨æ•°: {paper.citation_count}\n"
                if paper.influential_count > 0:
                    papers_text += f"é«˜å½±å“åŠ›å¼•ç”¨: {paper.influential_count}\n"
            papers_text += "---\n\n"
    
    # æ„å»ºæ•°æ®æºç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨metadataæ ‡ç­¾ï¼Œé¿å…AIæ³¨æ„åŠ›å‘æ•£ï¼‰
    source_stats = ""
    if source_results:
        total_from_all_sources = sum(len(r.papers) for r in source_results)
        source_stats = "\n<metadata>\n"
        source_stats += "**æ•°æ®æºç»Ÿè®¡ä¿¡æ¯ï¼ˆå…ƒæ•°æ®ï¼Œä»…ä¾›äº†è§£æœç´¢è¦†ç›–å¹¿åº¦ï¼Œæ— éœ€åœ¨æŠ¥å‘Šä¸­è¯¦ç»†åˆ†æï¼‰ï¼š**\n"
        for r in source_results:
            source_stats += f"- {r.source_name}: {len(r.papers)} æ¡\n"
        source_stats += f"- æ€»è®¡: {total_from_all_sources} æ¡ï¼ˆæ‰€æœ‰æ•°æ®æºï¼‰\n"
        source_stats += f"- ç»è¿‡è¯„åˆ†ç­›é€‰åï¼Œä»¥ä¸‹ {len(scored_papers)} ç¯‡è¿›å…¥æœ€ç»ˆåˆ†æ\n"
        source_stats += "</metadata>\n\n"
        source_stats += "**ä½ çš„ä¸»è¦ä»»åŠ¡ï¼šè§£æä¸‹æ–¹åˆ—å‡ºçš„è®ºæ–‡æ¸…å•ï¼Œç”Ÿæˆæ¯æ—¥æƒ…æŠ¥å†…å‚ã€‚ç»Ÿè®¡æ•°æ®ä»…ä¾›å‚è€ƒï¼Œæ— éœ€åœ¨æŠ¥å‘Šä¸­çº ç»“ä¸ºä»€ä¹ˆåªé€‰äº†è¿™å‡ ç¯‡ã€‚**\n\n"
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""ä½ æ˜¯ä¸€åä¸“æ³¨äºç”Ÿç‰©åŒ–å­¦ä¸åˆ†å­ç”Ÿç‰©å­¦ç ”ç©¶çš„ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯è¿‡å» 7 å¤©å†…ä»å¤šä¸ªæ•°æ®æºæŠ“å–çš„å­¦æœ¯åŠ¨æ€ï¼ŒåŒ…æ‹¬ï¼š
- é¡¶çº§è®ºæ–‡ï¼ˆbioRxivã€PubMedã€Europe PMCã€é¡¶åˆŠ RSSï¼‰
- ç§‘å­¦æ–°é—»ï¼ˆEurekAlertï¼‰
- æŠ€æœ¯å·¥å…·ï¼ˆGitHub æ–°ä»“åº“ï¼‰
- é«˜å½±å“åŠ›è®ºæ–‡ï¼ˆSemantic Scholarï¼‰

{source_stats}

**é‡è¦ï¼šæ£€ç´¢èŒƒå›´ä»…é™ä»¥ä¸‹ä¸‰ä¸ªç ”ç©¶æ–¹å‘ï¼Œå…¶ä»–æ–¹å‘çš„ç ”ç©¶è¯·å¿½ç•¥ï¼š**
1. ç”Ÿç‰©å›ºæ°®ï¼ˆBiological Nitrogen Fixationï¼‰
2. èƒå¤–ä¿¡å·æ„ŸçŸ¥ä¸ä¼ é€’ï¼ˆExtracellular Signal Perception and Transductionï¼ŒåŒ…å«ç»†èƒè†œè¡¨é¢å—ä½“/PRR/RLK ä»‹å¯¼çš„ PTI ç­‰æ¤ç‰©å…ç–«ä¿¡å·ï¼‰
3. é…¶çš„ç»“æ„ä¸ä½œç”¨æœºåˆ¶ï¼ˆEnzyme Structure and Mechanismï¼‰

ç ”ç©¶æ–¹å‘ï¼š{Config.RESEARCH_INTEREST}

**æ£€ç´¢èŒƒå›´ä¸¥æ ¼é™åˆ¶ï¼šä»…é™ä»¥ä¸Šä¸‰ä¸ªç ”ç©¶æ–¹å‘ï¼Œå…¶ä»–æ–¹å‘çš„ç ”ç©¶è¯·å¿½ç•¥ã€‚**

**è¯„åˆ†è¯´æ˜ï¼š**
- è®ºæ–‡å·²æŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åºï¼Œè¯„åˆ†è¶Šé«˜è¡¨ç¤ºä¸ä¸‰å¤§æ–¹å‘çš„å…³è”æ€§è¶Šå¼º
- P0ï¼ˆè¯„åˆ†â‰¥50åˆ†ï¼‰ï¼šçªç ´æ€§å·¥ä½œï¼Œä¸ä¸‰å¤§æ–¹å‘é«˜åº¦ç›¸å…³
- P1ï¼ˆè¯„åˆ†30-50åˆ†ï¼‰ï¼šé‡è¦ç ”ç©¶ï¼Œä¸ä¸‰å¤§æ–¹å‘ç›¸å…³
- P2ï¼ˆè¯„åˆ†<30åˆ†ï¼‰ï¼šä¸€èˆ¬ç ”ç©¶ï¼Œå¯èƒ½ä¸ä¸‰å¤§æ–¹å‘ç›¸å…³
- æ¯ç¯‡è®ºæ–‡çš„"è¯„åˆ†ç†ç”±"è¯´æ˜äº†ä¸ºä»€ä¹ˆå®ƒè¢«é€‰ä¸­ï¼Œè¯·é‡ç‚¹å…³æ³¨ä¸ä¸‰å¤§æ–¹å‘ç›¸å…³çš„ç†ç”±

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¾“å‡ºä¸€ä»½"æ¯æ—¥æƒ…æŠ¥å†…å‚"ï¼š

## ğŸ”¥ é¡¶çº§è®ºæ–‡ (P0 - æœ€é«˜ä¼˜å…ˆçº§)
**ä»·å€¼ï¼šå†³å®šä½ çš„è¯¾é¢˜æ˜¯å¦ä¼šè¢«"æŠ¢å‘"**

è¯†åˆ«å‡ºæ¶‰åŠã€ç”Ÿç‰©å›ºæ°®ã€èƒå¤–ä¿¡å·æ„ŸçŸ¥ä¸ä¼ é€’ã€é…¶çš„ç»“æ„ä¸ä½œç”¨æœºåˆ¶ã€‘çš„é‡ç£…æ–‡ç« ã€‚
å¯¹äºæ¯ç¯‡é‡ç£…æ–‡ç« ï¼Œè¯·ç”¨ä¸­æ–‡æ·±åº¦æ€»ç»“ï¼š
- ç ”ç©¶ç›®æ ‡/èƒŒæ™¯
- æ ¸å¿ƒå‘ç°/çªç ´
- å¯¹ç›¸å…³ç ”ç©¶é¢†åŸŸçš„æ„ä¹‰
- æ½œåœ¨åº”ç”¨ä»·å€¼
- **å¿…é¡»æ ‡æ³¨æœŸåˆŠæ¥æºå’Œå‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼ï¼šæ ‡é¢˜ (æœŸåˆŠ: XXX, å‘å¸ƒæ—¥æœŸ: YYYY-MM-DD)**
- é™„ä¸Š DOI æˆ–é“¾æ¥

## ğŸ› ï¸ æ–°å·¥å…· (P1 - æŠ€æœ¯å‰å“¨)
**ä»·å€¼ï¼šæé«˜ä½ å¤„ç†æ•°æ®æˆ–åºåˆ—åˆ†æçš„æ•ˆç‡**

å¦‚æœå‘ç° GitHub ä¸Šçš„æ–°ç®—æ³•ã€AlphaFold æ–°æ¨¡å‹ã€ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·ç­‰ï¼Œè¯·æ€»ç»“ï¼š
- å·¥å…·åç§°å’ŒåŠŸèƒ½
- å¯¹ç›¸å…³ç ”ç©¶çš„æ½œåœ¨åº”ç”¨
- é“¾æ¥åœ°å€
- **å¿…é¡»æ ‡æ³¨æ¥æºå¹³å°å’Œå‘å¸ƒ/æ›´æ–°æ—¥æœŸ**

## ğŸ’¡ å…³è”æ€§æŒ–æ˜ (P2 - å¯å‘ä»·å€¼)
**ä»·å€¼ï¼šä»éç›´æ¥ç ”ç©¶ä¸­å‘ç°å¯å€Ÿé‰´çš„æ–¹æ³•**

å¦‚æœæœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æ²¡æœ‰ç›´æ¥å±äºä¸‰å¤§æ–¹å‘çš„æ ¸å¿ƒçªç ´ï¼Œè¯·åˆ†ææ˜¯å¦æœ‰å…¶ä»–æ–¹å‘çš„ç ”ç©¶å¯ä»¥å€Ÿé‰´ï¼ˆä½†å¿…é¡»ä¸ä¸‰å¤§æ–¹å‘ç›¸å…³ï¼‰ï¼š
- ç»“æ„ç”Ÿç‰©å­¦æ–¹æ³•ï¼ˆå¯åº”ç”¨åˆ°é…¶æœºåˆ¶ç ”ç©¶ï¼‰
- ä¿¡å·è½¬å¯¼æœºåˆ¶ç ”ç©¶ï¼ˆå¯åº”ç”¨åˆ°èƒå¤–ä¿¡å·æ„ŸçŸ¥ï¼‰
- å¾®ç”Ÿç‰©ç»„å·¥ç¨‹åŒ–æ”¹é€ ï¼ˆå¯å¯å‘å›ºæ°®ç ”ç©¶ï¼‰

**é‡è¦æç¤ºï¼šå¦‚æœæ²¡æœ‰å‘ç°ç›´æ¥å±äºä¸‰å¤§æ–¹å‘çš„æ ¸å¿ƒè®ºæ–‡ï¼Œè¯·æ˜ç¡®è¯´æ˜"æœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æœªå‘ç°ç›´æ¥å±äºä¸‰å¤§æ–¹å‘çš„æ ¸å¿ƒæ›´æ–°"ï¼Œç„¶åé‡ç‚¹è¿›è¡Œå…³è”æ€§æŒ–æ˜åˆ†æã€‚ä¸è¦ç®€å•åœ°è¯´"ä»Šæ—¥æ— æ›´æ–°"ï¼Œå› ä¸ºå¯èƒ½æ˜¯æ£€ç´¢èŒƒå›´æˆ–æ•°æ®æºçš„é—®é¢˜ã€‚**

## ğŸ“Š æ•°æ®ç»Ÿè®¡ (P3 - è¶‹åŠ¿åˆ†æ)
**ä»·å€¼ï¼šäº†è§£é¢†åŸŸåŠ¨æ€**

ç®€è¦ç»Ÿè®¡ï¼š
- æœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æ–°å¢è®ºæ–‡æ•°ï¼ˆæ³¨æ„ï¼šè¿™é‡ŒæŒ‡çš„æ˜¯ç»è¿‡ç­›é€‰åçš„Topè®ºæ–‡æ•°é‡ï¼Œä¸æ˜¯æ‰€æœ‰æ•°æ®æºçš„æ€»æ•°ï¼‰
- ä¸»è¦æ¥æºåˆ†å¸ƒï¼ˆæ ¹æ®ä¸‹æ–¹åˆ—å‡ºçš„è®ºæ–‡æ¥æºç»Ÿè®¡ï¼Œä¸æ˜¯æ‰€æœ‰æ•°æ®æºçš„å®Œæ•´ç»Ÿè®¡ï¼‰
- ç ”ç©¶çƒ­ç‚¹å…³é”®è¯ï¼ˆåŸºäºä¸‹æ–¹åˆ—å‡ºçš„è®ºæ–‡åˆ†æï¼‰

{papers_text}

è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ï¼Œè¯¦ç»†é˜è¿°è¿™äº›è®ºæ–‡çš„ç§‘å­¦ä»·å€¼å’Œç ”ç©¶æ„ä¹‰ã€‚å­—æ•°è¦æ±‚ï¼š1000-2000å­—ã€‚

**ç‰¹åˆ«æé†’ï¼šå¦‚æœæœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æ²¡æœ‰ç›´æ¥å±äºä¸‰å¤§æ–¹å‘çš„æ ¸å¿ƒè®ºæ–‡ï¼Œè¯·æ˜ç¡®è¯´æ˜"æœ¬æ¬¡æ£€ç´¢èŒƒå›´å†…æœªå‘ç°ç›´æ¥å±äºä¸‰å¤§æ–¹å‘çš„æ ¸å¿ƒæ›´æ–°"ï¼Œç„¶åé‡ç‚¹è¿›è¡Œè·¨é¢†åŸŸçš„å…³è”æ€§æŒ–æ˜å’Œå¯å‘åˆ†æã€‚ä¸è¦ä½¿ç”¨"ä»Šæ—¥æ— æ›´æ–°"æˆ–"æ— é‡å¤§å…¬å¼€æ›´æ–°"è¿™æ ·çš„è¡¨è¿°ã€‚**
"""
    
    # è·å–æ‰€æœ‰ API å¯†é’¥ï¼ˆä¸»å¯†é’¥ + å¤‡ç”¨å¯†é’¥ï¼‰
    all_api_keys = Config.get_all_api_keys()
    logger.info(f"å¯ç”¨ API å¯†é’¥æ•°é‡: {len(all_api_keys)} (ä¸»å¯†é’¥ + {len(all_api_keys)-1} ä¸ªå¤‡ç”¨)")
    
    # éå†æ‰€æœ‰ API å¯†é’¥ï¼Œå¦‚æœå½“å‰å¯†é’¥å¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª
    last_error = None
    for key_index, api_key in enumerate(all_api_keys):
        key_name = "ä¸»å¯†é’¥" if key_index == 0 else f"å¤‡ç”¨å¯†é’¥{key_index}"
        masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "***"
        logger.info(f"å°è¯•ä½¿ç”¨ {key_name}: {masked_key}")
        
        # æ¯ä¸ªå¯†é’¥çš„é‡è¯•æœºåˆ¶ï¼ˆä½¿ç”¨é…ç½®çš„é‡è¯•æ¬¡æ•°ï¼‰
        max_retries_per_key = Config.API_MAX_RETRIES
        for attempt in range(max_retries_per_key):
            try:
                logger.info(f"æ­£åœ¨è°ƒç”¨AIç”ŸæˆæŠ¥å‘Šï¼ˆ{key_name}, ç¬¬ {attempt + 1}/{max_retries_per_key} æ¬¡å°è¯•ï¼‰...")
                logger.info(f"APIåœ°å€: {Config.DEEPSEEK_BASE_URL}, è¶…æ—¶è®¾ç½®: {Config.API_TIMEOUT}ç§’")
                
                # æ¯æ¬¡é‡è¯•éƒ½åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹ï¼Œé¿å…è¿æ¥å¤ç”¨é—®é¢˜
                client = OpenAI(
                    api_key=api_key,
                    base_url=Config.DEEPSEEK_BASE_URL,
                    timeout=Config.API_TIMEOUT,
                    max_retries=0  # ç¦ç”¨ SDK å†…ç½®é‡è¯•ï¼Œæ‰‹åŠ¨æ§åˆ¶
                )
                
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç”Ÿç‰©åŒ–å­¦ä¸åˆ†å­ç”Ÿç‰©å­¦é¢†åŸŸçš„ä¸“å®¶ï¼Œæ“…é•¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£è¯»å­¦æœ¯è®ºæ–‡ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=3000
                )
                
                daily_report = response.choices[0].message.content
                logger.info(f"âœ… AIæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼(ä½¿ç”¨ {key_name})")
                
                # æ·»åŠ æ•°æ®å®Œæ•´æ€§æç¤º
                if source_results:
                    degraded_sources = [sr for sr in source_results if sr.is_degraded or sr.error]
                    if degraded_sources:
                        daily_report += "\n\n## âš ï¸ æ•°æ®å®Œæ•´æ€§è¯´æ˜\n\n"
                        daily_report += "æœ¬æ¬¡æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­,ä»¥ä¸‹æ•°æ®æºæœªèƒ½å®Œæ•´è·å–:\n"
                        for sr in degraded_sources:
                            if sr.error:
                                daily_report += f"- {sr.source_name}: æŠ½å–å¤±è´¥ ({sr.error})\n"
                            elif sr.degraded_reason:
                                daily_report += f"- {sr.source_name}: {sr.degraded_reason}\n"
                        daily_report += "\nå»ºè®®å…³æ³¨åç»­æ›´æ–°,æˆ–æ‰‹åŠ¨è®¿é—®å¯¹åº”æ•°æ®æºç¡®è®¤ã€‚\n"
                
                return daily_report
            except KeyboardInterrupt:
                logger.warning("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­äº†ç¨‹åº")
                raise
            except Exception as e:
                last_error = e
                error_type = type(e).__name__
                error_msg = str(e)
                logger.error(f"è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹æ—¶å‡ºé”™ï¼ˆ{key_name}, ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰: {error_type}: {error_msg[:200]}")
                
                # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                import traceback
                logger.debug(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                
                # å¦‚æœæ˜¯è®¤è¯é”™è¯¯ï¼ˆ401ï¼‰ï¼Œç›´æ¥åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥
                if "401" in error_msg or "Unauthorized" in error_msg or "invalid" in error_msg.lower():
                    logger.warning(f"{key_name} è®¤è¯å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥")
                    break  # è·³å‡ºå½“å‰å¯†é’¥çš„é‡è¯•å¾ªç¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥
                
                # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥é‡è¯•
                if attempt < max_retries_per_key - 1:
                    wait_time = min(Config.API_RETRY_BASE_DELAY * (2 ** attempt), Config.API_RETRY_MAX_DELAY)
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰...")
                    time.sleep(wait_time)
                else:
                    # å½“å‰å¯†é’¥çš„æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥
                    logger.warning(f"{key_name} æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥")
                    if key_index < len(all_api_keys) - 1:
                        time.sleep(2)  # åˆ‡æ¢å¯†é’¥å‰ç¨ä½œç­‰å¾…
                    break  # è·³å‡ºå½“å‰å¯†é’¥çš„é‡è¯•å¾ªç¯
    
    # æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥ï¼Œè¿”å›é™çº§æŠ¥å‘Š
    logger.error(f"æ‰€æœ‰ API å¯†é’¥ï¼ˆå…± {len(all_api_keys)} ä¸ªï¼‰å‡å¤±è´¥ï¼Œè¿”å›é™çº§æŠ¥å‘Š")
    return _generate_fallback_report(scored_papers, last_error)



